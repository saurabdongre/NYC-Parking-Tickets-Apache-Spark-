{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"NYC_Parking_CS\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading and loading the CSV file\n",
    "d_frame = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons Number|Plate ID|Registration State|         Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|    5092469481| GZH7067|                NY|2016-07-10 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0143A|\n",
      "|    5092451658| GZH7067|                NY|2016-07-08 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0400P|\n",
      "|    4006265037| FZX9232|                NY|2016-08-23 00:00:00|             5|             SUBN|        FORD|                 0|              0|         0233P|\n",
      "|    8478629828| 66623ME|                NY|2017-06-14 00:00:00|            47|             REFG|       MITSU|                14|             14|         1120A|\n",
      "|    7868300310| 37033JV|                NY|2016-11-21 00:00:00|            69|             DELV|       INTER|                13|             13|         0555P|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show\n",
    "d_frame.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assumption: Assuming that the whole data belongs to the year 2017. We will not filter anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Summons Number',\n",
       " 'Plate ID',\n",
       " 'Registration State',\n",
       " 'Issue Date',\n",
       " 'Violation Code',\n",
       " 'Vehicle Body Type',\n",
       " 'Vehicle Make',\n",
       " 'Violation Precinct',\n",
       " 'Issuer Precinct',\n",
       " 'Violation Time']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all the columns \n",
    "d_frame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the columns seems to have space in their names, lets have them rectified, lets rename them.... \n",
    "\n",
    "d_frame = d_frame.withColumnRenamed(\"Summons Number\", \"Summons_Number\")\n",
    "d_frame = d_frame.withColumnRenamed(\"Plate ID\",\"Plate_ID\")\n",
    "d_frame = d_frame.withColumnRenamed(\"Registration State\", \"Registration_State\")\n",
    "d_frame = d_frame.withColumnRenamed(\"Issue Date\", \"Issue_Date\")\n",
    "d_frame = d_frame.withColumnRenamed(\"Violation Code\", \"Violation_Code\")\n",
    "d_frame = d_frame.withColumnRenamed(\"Vehicle Body Type\", \"Vehicle_Body_Type\")\n",
    "d_frame = d_frame.withColumnRenamed(\"Vehicle Make\", \"Vehicle_Make\")\n",
    "d_frame = d_frame.withColumnRenamed(\"Violation Precinct\", \"Violation_Precinct\")\n",
    "d_frame = d_frame.withColumnRenamed(\"Issuer Precinct\", \"Issuer_Precinct\")\n",
    "d_frame = d_frame.withColumnRenamed(\"Violation Time\", \"Violation_Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Summons_Number,LongType,true),StructField(Plate_ID,StringType,true),StructField(Registration_State,StringType,true),StructField(Issue_Date,TimestampType,true),StructField(Violation_Code,IntegerType,true),StructField(Vehicle_Body_Type,StringType,true),StructField(Vehicle_Make,StringType,true),StructField(Violation_Precinct,IntegerType,true),StructField(Issuer_Precinct,IntegerType,true),StructField(Violation_Time,StringType,true)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examining\n",
    "d_frame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------+------------------+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|summary|     Summons_Number|Plate_ID|Registration_State|    Violation_Code| Vehicle_Body_Type|      Vehicle_Make|Violation_Precinct|  Issuer_Precinct|   Violation_Time|\n",
      "+-------+-------------------+--------+------------------+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|  count|           10803028|10803028|          10803028|          10803028|          10803028|          10803028|          10803028|         10803028|         10803028|\n",
      "|   mean|6.817447029065788E9|Infinity|              99.0|34.599430455979565|3.9258887134586864| 6519.974025974026| 45.01216260848347|46.82931211508477|909.2857142857143|\n",
      "| stddev|2.320233962328227E9|     NaN|               0.0|19.359868716323483|0.5013415469252528|18091.257389147086|40.552560268435705|62.66703577269572|791.8453853409226|\n",
      "|    min|         1002884949|   #1MOM|                99|                 0|                00|             ,FREI|                 0|                0|            .240P|\n",
      "|    max|         8585600044|       ~|                WY|                99|               nan|               nan|               933|              997|              nan|\n",
      "+-------+-------------------+--------+------------------+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show\n",
    "d_frame.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Summons_Number: long (nullable = true)\n",
      " |-- Plate_ID: string (nullable = true)\n",
      " |-- Registration_State: string (nullable = true)\n",
      " |-- Issue_Date: timestamp (nullable = true)\n",
      " |-- Violation_Code: integer (nullable = true)\n",
      " |-- Vehicle_Body_Type: string (nullable = true)\n",
      " |-- Vehicle_Make: string (nullable = true)\n",
      " |-- Violation_Precinct: integer (nullable = true)\n",
      " |-- Issuer_Precinct: integer (nullable = true)\n",
      " |-- Violation_Time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#scheman view\n",
    "d_frame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### checking null values, column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan\n",
    "print(d_frame.filter((d_frame[\"Summons_Number\"] == \"\") | d_frame[\"Summons_Number\"].isNull() | isnan(d_frame[\"Summons_Number\"])).count())\n",
    "print(d_frame.filter((d_frame[\"Registration_State\"] == \"\") | d_frame[\"Registration_State\"].isNull() | isnan(d_frame[\"Registration_State\"])).count())\n",
    "print(d_frame.filter((d_frame[\"Plate_ID\"] == \"\") | d_frame[\"Plate_ID\"].isNull() | isnan(d_frame[\"Plate_ID\"])).count())\n",
    "print(d_frame.filter((d_frame[\"Violation_Code\"] == \"\") | d_frame[\"Violation_Code\"].isNull() | isnan(d_frame[\"Violation_Code\"])).count())\n",
    "print(d_frame.filter((d_frame[\"Vehicle_Body_Type\"] == \"\") | d_frame[\"Vehicle_Body_Type\"].isNull() | isnan(d_frame[\"Vehicle_Body_Type\"])).count())\n",
    "print(d_frame.filter((d_frame[\"Vehicle_Make\"] == \"\") | d_frame[\"Vehicle_Make\"].isNull() | isnan(d_frame[\"Vehicle_Make\"])).count())\n",
    "print(d_frame.filter((d_frame[\"Violation_Precinct\"] == \"\") | d_frame[\"Violation_Precinct\"].isNull() | isnan(d_frame[\"Violation_Precinct\"])).count())\n",
    "print(d_frame.filter((d_frame[\"Issuer_Precinct\"] == \"\") | d_frame[\"Issuer_Precinct\"].isNull() | isnan(d_frame[\"Issuer_Precinct\"])).count())\n",
    "print(d_frame.filter((d_frame[\"Violation_Time\"] == \"\") | d_frame[\"Violation_Time\"].isNull() | isnan(d_frame[\"Violation_Time\"])).count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### looks like there are no null values in the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping off NAs if any\n",
    "d_frame = d_frame.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the data(*Questions*)\n",
    "#### 1. Find the total number of tickets for the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10803028"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "d_frame.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After this checkpoint - we would require SQL queries, so to execute them, lets create a temp tableView.\n",
    "d_frame.createOrReplaceTempView(\"temp_ViewTickets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Find out the number of unique states from where the cars that got parking tickets came. \n",
    "    (Hint: Use the column 'Registration State'.)\n",
    "     There is a numeric entry '99' in the column, which should be corrected. \n",
    "       Replace it with the state having the maximum entries. Provide the number of unique states again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states with tickets: \n",
      "+---------------------------+\n",
      "|states_tickets_unique_count|\n",
      "+---------------------------+\n",
      "|                         67|\n",
      "+---------------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of states with tickets: \")\n",
    "states_tickets = spark.sql(\"SELECT count(distinct(Registration_State)) as states_tickets_unique_count FROM temp_ViewTickets\")\n",
    "print(states_tickets.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36625"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tried writting the entire query in one cell, but it took quite a lot of time, so please refer below cells for th same...\n",
    "#as per question, numeric etry '99' needs to be corrected, lets have a count\n",
    "\n",
    "# Filter where Registration_State = \"99\"\n",
    "from pyspark.sql.functions import col\n",
    "temp_df = d_frame.where(col(\"Registration_State\") == \"99\")\n",
    "temp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "|Registration_State|total_count|\n",
      "+------------------+-----------+\n",
      "|                NY|    8481061|\n",
      "|                NJ|     925965|\n",
      "|                PA|     285419|\n",
      "|                FL|     144556|\n",
      "|                CT|     141088|\n",
      "+------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to replace '99' lets count the hoghest ticketing state\n",
    "states_count = spark.sql(\"SELECT Registration_State, count(*) as total_count FROM temp_ViewTickets group by Registration_State order by total_count desc\")\n",
    "states_count.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so its NewYork, replacing 99 with NY\n",
    "d_frame = d_frame.na.replace(['99'], ['NY'], 'Registration_State')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again the change needs to be encorporated, replacing the existing viewtable again\n",
    "d_frame.createOrReplaceTempView(\"temp_ViewTickets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|unique_states_count|\n",
      "+-------------------+\n",
      "|                 66|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Below is the answer to the Que : Find out the number of unique states from where the cars that got parking tickets came. \n",
    "Unique_states_count = spark.sql(\"SELECT count(distinct(Registration_State)) as unique_states_count FROM temp_ViewTickets\")\n",
    "Unique_states_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aggregation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How often does each violation code occur? Display the frequency of the top five violation codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|Violation_Code|total_count|\n",
      "+--------------+-----------+\n",
      "|            21|    1528588|\n",
      "|            36|    1400614|\n",
      "|            38|    1062304|\n",
      "|            14|     893498|\n",
      "|            20|     618593|\n",
      "+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voilation_code = spark.sql(\"SELECT Violation_Code, count(*) as total_count FROM temp_ViewTickets group by Violation_Code order by total_count desc\")\n",
    "voilation_code.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. How often does each 'vehicle body type' get a parking ticket? How about the 'vehicle make'? \n",
    "  (Hint: Find the top 5 for both.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How often does each 'vehicle body type' get a parking ticket?\n",
      "+-----------------+-----------+\n",
      "|Vehicle_Body_Type|total_count|\n",
      "+-----------------+-----------+\n",
      "|             SUBN|    3719802|\n",
      "|             4DSD|    3082020|\n",
      "|              VAN|    1411970|\n",
      "|             DELV|     687330|\n",
      "|              SDN|     438191|\n",
      "+-----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "How about the 'vehicle make'?\n",
      "+------------+-----------+\n",
      "|Vehicle_Make|total_count|\n",
      "+------------+-----------+\n",
      "|        FORD|    1280958|\n",
      "|       TOYOT|    1211451|\n",
      "|       HONDA|    1079238|\n",
      "|       NISSA|     918590|\n",
      "|       CHEVR|     714655|\n",
      "+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"How often does each 'vehicle body type' get a parking ticket?\")\n",
    "how_often = spark.sql(\"SELECT Vehicle_Body_Type, count(*) as total_count FROM temp_ViewTickets group by Vehicle_Body_Type order by total_count desc\")\n",
    "print(how_often.show(5))\n",
    "\n",
    "\n",
    "print(\"How about the 'vehicle make'?\")\n",
    "v_make = spark.sql(\"SELECT Vehicle_Make, count(*) as total_count FROM temp_ViewTickets group by Vehicle_Make order by total_count desc\")\n",
    "v_make.show(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. A precinct is a police station that has a certain zone of the city under its command. Find the (5 highest) frequencies of tickets for each of the following:\n",
    "'Violation Precinct' (This is the precinct of the zone where the violation occurred). Using this, can you draw any insights for parking violations in any specific areas of the city?\n",
    "'Issuer Precinct' (This is the precinct that issued the ticket.)\n",
    "Here, you would have noticed that the dataframe has the'Violating Precinct' or 'Issuing Precinct' as '0'. \n",
    "These are erroneous entries. Hence, you need to provide the records for five correct precincts. (Hint: Print the top six entries after sorting.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "|Violation_Precinct|total_count|\n",
      "+------------------+-----------+\n",
      "|                 0|    2072400|\n",
      "|                19|     535671|\n",
      "|                14|     352450|\n",
      "|                 1|     331810|\n",
      "|                18|     306920|\n",
      "|               114|     296514|\n",
      "+------------------+-----------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_v_precinct = spark.sql(\"SELECT Violation_Precinct, count(*) as total_count FROM temp_ViewTickets group by Violation_Precinct order by total_count desc\")\n",
    "top_v_precinct.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "|Issuer_Precinct|total_count|\n",
      "+---------------+-----------+\n",
      "|              0|    2388479|\n",
      "|             19|     521513|\n",
      "|             14|     344977|\n",
      "|              1|     321170|\n",
      "|             18|     296553|\n",
      "|            114|     289950|\n",
      "+---------------+-----------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Issuing Precincts (this is the precinct that issued the ticket)\n",
    "\n",
    "top_i_precinct = spark.sql(\"SELECT Issuer_Precinct, count(*) as total_count FROM temp_ViewTickets group by Issuer_Precinct order by total_count desc\")\n",
    "top_i_precinct.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Find the violation code frequencies for three precincts that have issued the most number of tickets. \n",
    "   Do these precinct zones have an exceptionally high frequency of certain violation codes? \n",
    "   Are these codes common across precincts? \n",
    "   (Hint: In the SQL view, use the 'where' attribute to filter among three precincts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+-------------+\n",
      "|Issuer_Precinct|Violation_Code|total_tickets|\n",
      "+---------------+--------------+-------------+\n",
      "|             14|            14|        73837|\n",
      "|             14|            69|        58026|\n",
      "|             14|            31|        39857|\n",
      "|             14|            47|        30540|\n",
      "|             14|            42|        20663|\n",
      "+---------------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------+--------------+-------------+\n",
      "|Issuer_Precinct|Violation_Code|total_tickets|\n",
      "+---------------+--------------+-------------+\n",
      "|             19|            46|        86390|\n",
      "|             19|            37|        72437|\n",
      "|             19|            38|        72344|\n",
      "|             19|            14|        57563|\n",
      "|             19|            21|        54700|\n",
      "+---------------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------+--------------+-------------+\n",
      "|Issuer_Precinct|Violation_Code|total_tickets|\n",
      "+---------------+--------------+-------------+\n",
      "|              1|            14|        73522|\n",
      "|              1|            16|        38937|\n",
      "|              1|            20|        27841|\n",
      "|              1|            46|        22534|\n",
      "|              1|            38|        16989|\n",
      "+---------------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#It looks like '14', '19' and '1' are the top 3 precincts which were issued maximum tickets, so taking them into act..\n",
    "issue_14 = spark.sql(\"SELECT Issuer_Precinct, Violation_Code, count(*) as total_tickets FROM temp_ViewTickets where Issuer_Precinct = '14' group by Issuer_Precinct, Violation_Code order by total_tickets desc\")\n",
    "issue_14.show(5)\n",
    "\n",
    "issue_19 = spark.sql(\"SELECT Issuer_Precinct, Violation_Code, count(*) as total_tickets FROM temp_ViewTickets where Issuer_Precinct = '19' group by Issuer_Precinct, Violation_Code order by total_tickets desc\")\n",
    "issue_19.show(5)\n",
    "\n",
    "issue_1 = spark.sql(\"SELECT Issuer_Precinct, Violation_Code, count(*) as total_tickets FROM temp_ViewTickets where Issuer_Precinct = '1' group by Issuer_Precinct, Violation_Code order by total_tickets desc\")\n",
    "issue_1.show(5)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "######  For certain zones the precinct is high as compared above, and they are common accross, and we can say that the most common violation code is \"14\".    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Find out the properties of parking violations across different times of the day:\n",
    "    \n",
    "  Find a way to deal with missing values, if any.\n",
    "  (Hint: Check for the null values using 'isNull' under the SQL. Also, to remove the null values, check the 'dropna' command in the API documentation.)\n",
    "\n",
    "The Violation Time field is specified in a strange format. Find a way to make this a time attribute that you can use to divide into groups.\n",
    "Divide 24 hours into six equal discrete bins of time. Choose the intervals as you see fit. \n",
    "For each of these groups, find the three most commonly occurring violations.\n",
    "(Hint: Use the CASE-WHEN in SQL view to segregate into bins. To find the most commonly occurring violations, you can use an approach similar to the one mentioned in the hint for question 4.)\n",
    "\n",
    "Now, try another direction. For the three most commonly occurring violation codes, find the most common time of the day (in terms of the bins from the previous part)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|null_time_count|\n",
      "+---------------+\n",
      "|              0|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#First of all, checking for null values..\n",
    "null_times = spark.sql(\"select count(*) as null_time_count FROM temp_ViewTickets where Violation_Time is Null\")\n",
    "null_times.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### there are no null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Violation_Time|\n",
      "+--------------+\n",
      "|         0143A|\n",
      "|         0400P|\n",
      "|         0233P|\n",
      "|         1120A|\n",
      "|         0555P|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Further look into the column\n",
    "violacing_column = spark.sql(\"select Violation_Time from temp_ViewTickets\")\n",
    "violacing_column.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wierd looking time reported, lets make changes to that first.\n",
    "well this particular question is huge and i don't want to replace the violatopn_time column with the new refined column in the existing d_frame.\n",
    "So lets create a new d_frame just for the time, and find the analysis just for that, only for this question.\n",
    "As stated in the queton moving to the 24 hour clock as PM and AM in the new data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|Violation_Time|Refined_Violation_Time|Violation_Code|\n",
      "+--------------+----------------------+--------------+\n",
      "|         0143A|                 01:43|             7|\n",
      "|         0400P|                 16:00|             7|\n",
      "|         0233P|                 14:33|             5|\n",
      "|         1120A|                 11:20|            47|\n",
      "|         0555P|                 17:55|            69|\n",
      "|         0852P|                 20:52|             7|\n",
      "|         0215A|                 02:15|            40|\n",
      "|         0758A|                 07:58|            36|\n",
      "|         1005A|                 10:05|            36|\n",
      "|         0845A|                 08:45|             5|\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_frame_Vtime = spark.sql(\"select Violation_Time, if( left(Violation_Time, 2) == '12' or right(Violation_Time, 1) == 'A', \\\n",
    "concat(substring(Violation_Time, 1,2),':', substring(Violation_Time, 3,2)), concat(int(substring(Violation_Time, 1,2)+12),':',\\\n",
    "substring(Violation_Time, 3,2))) as Refined_Violation_Time,Violation_Code from temp_ViewTickets\")\n",
    "    \n",
    "d_frame_Vtime.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we will be needing SQL for this lets create a tempView for SQL...\n",
    "d_frame_Vtime.createOrReplaceTempView(\"temp_ViewVTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|Violation_Time|Refined_Violation_Time|Violation_Code|\n",
      "+--------------+----------------------+--------------+\n",
      "|         0143A|                 01:43|             7|\n",
      "|         0400P|                 16:00|             7|\n",
      "|         0233P|                 14:33|             5|\n",
      "|         1120A|                 11:20|            47|\n",
      "|         0555P|                 17:55|            69|\n",
      "|         0852P|                 20:52|             7|\n",
      "|         0215A|                 02:15|            40|\n",
      "|         0758A|                 07:58|            36|\n",
      "|         1005A|                 10:05|            36|\n",
      "|         0845A|                 08:45|             5|\n",
      "|         0015A|                 00:15|            78|\n",
      "|         0707A|                 07:07|            19|\n",
      "|         1022A|                 10:22|            36|\n",
      "|         1150A|                 11:50|            21|\n",
      "|         0525A|                 05:25|            40|\n",
      "|         0645P|                 18:45|            71|\n",
      "|         1122A|                 11:22|             7|\n",
      "|         0256P|                 14:56|            64|\n",
      "|         1232A|                 12:32|            20|\n",
      "|         1034A|                 10:34|            36|\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from temp_ViewVTime\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------------+---------------+\n",
      "|Refined_Violation_Time|Violation_Code|bins_Refined_04|\n",
      "+----------------------+--------------+---------------+\n",
      "|                 01:43|             7|        [00-04]|\n",
      "|                 16:00|             7|        [12-16]|\n",
      "|                 14:33|             5|        [12-16]|\n",
      "|                 11:20|            47|        [08-12]|\n",
      "|                 17:55|            69|        [16-20]|\n",
      "|                 20:52|             7|        [16-20]|\n",
      "|                 02:15|            40|        [00-04]|\n",
      "|                 07:58|            36|        [04-08]|\n",
      "|                 10:05|            36|        [08-12]|\n",
      "|                 08:45|             5|        [04-08]|\n",
      "|                 00:15|            78|        [00-04]|\n",
      "|                 07:07|            19|        [04-08]|\n",
      "|                 10:22|            36|        [08-12]|\n",
      "|                 11:50|            21|        [08-12]|\n",
      "|                 05:25|            40|        [04-08]|\n",
      "|                 18:45|            71|        [16-20]|\n",
      "|                 11:22|             7|        [08-12]|\n",
      "|                 14:56|            64|        [12-16]|\n",
      "|                 12:32|            20|        [08-12]|\n",
      "|                 10:34|            36|        [08-12]|\n",
      "+----------------------+--------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now binning ... using case as mentioned in the question itself..\n",
    "\n",
    "bins_Refined = spark.sql(\"select Refined_Violation_Time, Violation_Code, case \\\n",
    "                        when int(substring(`Refined_Violation_Time`,1,2)) between 00 and 04 then '[00-04]'\\\n",
    "                        when int(substring(`Refined_Violation_Time`,1,2)) between 05 and 08 then '[04-08]'\\\n",
    "                        when int(substring(`Refined_Violation_Time`,1,2)) between 09 and 12 then '[08-12]'\\\n",
    "                        when int(substring(`Refined_Violation_Time`,1,2)) between 13 and 16 then '[12-16]'\\\n",
    "                        when int(substring(`Refined_Violation_Time`,1,2)) between 16 and 20 then '[16-20]'\\\n",
    "                        else '[20-24]'\\\n",
    "                        end as bins_Refined_04 from temp_ViewVTime\")\n",
    "\n",
    "bins_Refined.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets update the bins in the tempView\n",
    "bins_Refined.createOrReplaceTempView(\"temp_ViewVTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+\n",
      "|Violation_Code|total_counts|\n",
      "+--------------+------------+\n",
      "|            21|     1528588|\n",
      "|            36|     1400614|\n",
      "|            38|     1062304|\n",
      "+--------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#after binning lets try to find the most common voilating codes\n",
    "\n",
    "common_codes = spark.sql(\"SELECT Violation_Code, count(*) as total_counts FROM temp_ViewVTime group by Violation_Code order by total_counts desc\")\n",
    "common_codes.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for timeBin - [00-04]\n",
      "+---------------+--------------+-----------+\n",
      "|bins_Refined_04|Violation_Code|total_count|\n",
      "+---------------+--------------+-----------+\n",
      "|        [00-04]|            21|      74996|\n",
      "|        [00-04]|            40|      52000|\n",
      "|        [00-04]|            78|      31628|\n",
      "+---------------+--------------+-----------+\n",
      "\n",
      "for timeBin - [04-08]\n",
      "+---------------+--------------+-----------+\n",
      "|bins_Refined_04|Violation_Code|total_count|\n",
      "+---------------+--------------+-----------+\n",
      "|        [04-08]|            21|     498338|\n",
      "|        [04-08]|            14|     207958|\n",
      "|        [04-08]|            36|     170484|\n",
      "+---------------+--------------+-----------+\n",
      "\n",
      "for timeBin - [08-12]\n",
      "+---------------+--------------+-----------+\n",
      "|bins_Refined_04|Violation_Code|total_count|\n",
      "+---------------+--------------+-----------+\n",
      "|        [08-12]|            21|     949967|\n",
      "|        [08-12]|            36|     826311|\n",
      "|        [08-12]|            38|     402756|\n",
      "+---------------+--------------+-----------+\n",
      "\n",
      "for timeBin - [12-16]\n",
      "+---------------+--------------+-----------+\n",
      "|bins_Refined_04|Violation_Code|total_count|\n",
      "+---------------+--------------+-----------+\n",
      "|        [12-16]|            38|     453644|\n",
      "|        [12-16]|            36|     394403|\n",
      "|        [12-16]|            37|     339739|\n",
      "+---------------+--------------+-----------+\n",
      "\n",
      "for timeBin - [16-20]\n",
      "+---------------+--------------+-----------+\n",
      "|bins_Refined_04|Violation_Code|total_count|\n",
      "+---------------+--------------+-----------+\n",
      "|        [16-20]|            38|     124026|\n",
      "|        [16-20]|             7|     112971|\n",
      "|        [16-20]|            14|      78491|\n",
      "+---------------+--------------+-----------+\n",
      "\n",
      "for timeBin - [20-24]\n",
      "+---------------+--------------+-----------+\n",
      "|bins_Refined_04|Violation_Code|total_count|\n",
      "+---------------+--------------+-----------+\n",
      "|        [20-24]|             7|      45323|\n",
      "|        [20-24]|            40|      34707|\n",
      "|        [20-24]|            14|      33169|\n",
      "+---------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for each of the bin lets try find the top 3 like we did in the Question (4)\n",
    "\n",
    "bin1 = spark.sql(\"SELECT bins_Refined_04, Violation_Code, count(*) as total_count \\\n",
    "                    FROM temp_ViewVTime where bins_Refined_04 = '[00-04]' \\\n",
    "                    group by bins_Refined_04, Violation_Code \\\n",
    "                    order by total_count desc limit 3\")\n",
    "print(\"for timeBin - [00-04]\")\n",
    "bin1.show()\n",
    "\n",
    "#---------------------\n",
    "\n",
    "bin2 = spark.sql(\"SELECT bins_Refined_04, Violation_Code, count(*) as total_count \\\n",
    "                    FROM temp_ViewVTime where bins_Refined_04 = '[04-08]' \\\n",
    "                    group by bins_Refined_04, Violation_Code \\\n",
    "                    order by total_count desc limit 3\")\n",
    "print(\"for timeBin - [04-08]\")\n",
    "bin2.show()\n",
    "\n",
    "#---------------------\n",
    "\n",
    "bin3 = spark.sql(\"SELECT bins_Refined_04, Violation_Code, count(*) as total_count \\\n",
    "                    FROM temp_ViewVTime where bins_Refined_04 = '[08-12]' \\\n",
    "                    group by bins_Refined_04, Violation_Code \\\n",
    "                    order by total_count desc limit 3\")\n",
    "print(\"for timeBin - [08-12]\")\n",
    "bin3.show()\n",
    "\n",
    "#---------------------\n",
    "\n",
    "bin4 = spark.sql(\"SELECT bins_Refined_04, Violation_Code, count(*) as total_count \\\n",
    "                    FROM temp_ViewVTime where bins_Refined_04 = '[12-16]' \\\n",
    "                    group by bins_Refined_04, Violation_Code \\\n",
    "                    order by total_count desc limit 3\")\n",
    "print(\"for timeBin - [12-16]\")\n",
    "bin4.show()\n",
    "\n",
    "#---------------------\n",
    "\n",
    "bin5 = spark.sql(\"SELECT bins_Refined_04, Violation_Code, count(*) as total_count \\\n",
    "                    FROM temp_ViewVTime where bins_Refined_04 = '[16-20]' \\\n",
    "                    group by bins_Refined_04, Violation_Code \\\n",
    "                    order by total_count desc limit 3\")\n",
    "print(\"for timeBin - [16-20]\")\n",
    "bin5.show()\n",
    "\n",
    "#---------------------\n",
    "\n",
    "bin6 = spark.sql(\"SELECT bins_Refined_04, Violation_Code, count(*) as total_count \\\n",
    "                    FROM temp_ViewVTime where bins_Refined_04 = '[20-24]' \\\n",
    "                    group by bins_Refined_04, Violation_Code \\\n",
    "                    order by total_count desc limit 3\")\n",
    "print(\"for timeBin - [20-24]\")\n",
    "bin6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for code21\n",
      "+--------------+---------------+-----------+\n",
      "|Violation_Code|bins_Refined_04|total_count|\n",
      "+--------------+---------------+-----------+\n",
      "|            21|        [08-12]|     949967|\n",
      "+--------------+---------------+-----------+\n",
      "only showing top 1 row\n",
      "\n",
      "for code36\n",
      "+--------------+---------------+-----------+\n",
      "|Violation_Code|bins_Refined_04|total_count|\n",
      "+--------------+---------------+-----------+\n",
      "|            36|        [08-12]|     826311|\n",
      "+--------------+---------------+-----------+\n",
      "only showing top 1 row\n",
      "\n",
      "for code38\n",
      "+--------------+---------------+-----------+\n",
      "|Violation_Code|bins_Refined_04|total_count|\n",
      "+--------------+---------------+-----------+\n",
      "|            38|        [12-16]|     453644|\n",
      "+--------------+---------------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now as per the question, in the another approach since the 3 most common violation code: 21, 36, 38, lets have them sorted as well from previous part\n",
    "\n",
    "code21 = spark.sql(\"SELECT Violation_Code, bins_Refined_04, count(*) as total_count \\\n",
    "                    FROM temp_ViewVTime where Violation_Code = '21' group by Violation_Code, bins_Refined_04 \\\n",
    "                    order by total_count desc\")\n",
    "print(\"for code21\")\n",
    "code21.show(1)\n",
    "\n",
    "#---------------------\n",
    "\n",
    "code36 = spark.sql(\"SELECT Violation_Code, bins_Refined_04, count(*) as total_count \\\n",
    "                    FROM temp_ViewVTime where Violation_Code = '36' group by Violation_Code, bins_Refined_04 \\\n",
    "                    order by total_count desc\")\n",
    "print(\"for code36\")\n",
    "code36.show(1)\n",
    "\n",
    "#---------------------\n",
    "\n",
    "code38 = spark.sql(\"SELECT Violation_Code, bins_Refined_04, count(*) as total_count \\\n",
    "                    FROM temp_ViewVTime where Violation_Code = '38' group by Violation_Code, bins_Refined_04 \\\n",
    "                    order by total_count desc\")\n",
    "print(\"for code38\")\n",
    "code38.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Letâ€™s try and find some seasonality in this data:\n",
    "    \n",
    "   First, divide the year into a certain number of seasons, and find the frequencies of tickets for each season. \n",
    "        (Hint: Use Issue Date to segregate into seasons.)\n",
    "\n",
    "   Then, find the three most common violations for each of these seasons.\n",
    "        (Hint: You can use an approach similar to the one mentioned in the hint for question 4.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|Violation_Code|Issue_Date|\n",
      "+--------------+----------+\n",
      "+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Violation_Code, Issue_Date from temp_ViewTickets where month(to_date(Issue_Date, 'MM/dd/yyyy')) > 12\").show()        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data has no months greater than 12, thats good\n",
    "#### Since this is NY - USA we are talking about, so there could possibly be 4 seasons - Summer, Winter, Spring and Fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_frame_seasonsTickets = spark.sql(\"select Violation_Code, Issue_Date, case \\\n",
    "                    when month(to_date(Issue_Date, 'MM/dd/yyyy')) in (12,1,2)  then 'Winter' \\\n",
    "                    when month(to_date(Issue_Date, 'MM/dd/yyyy')) in (3,4,5)   then 'Spring' \\\n",
    "                    when month(to_date(Issue_Date, 'MM/dd/yyyy')) in (6,7,8)   then 'Summer' \\\n",
    "                    else 'Fall' \\\n",
    "                    end as NY_season from temp_ViewTickets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+---------+\n",
      "|Violation_Code|         Issue_Date|NY_season|\n",
      "+--------------+-------------------+---------+\n",
      "|             7|2016-07-10 00:00:00|   Summer|\n",
      "|             7|2016-07-08 00:00:00|   Summer|\n",
      "|             5|2016-08-23 00:00:00|   Summer|\n",
      "|            47|2017-06-14 00:00:00|   Summer|\n",
      "|            69|2016-11-21 00:00:00|     Fall|\n",
      "|             7|2017-06-13 00:00:00|   Summer|\n",
      "|            40|2016-08-03 00:00:00|   Summer|\n",
      "|            36|2016-12-21 00:00:00|   Winter|\n",
      "|            36|2016-11-21 00:00:00|     Fall|\n",
      "|             5|2016-10-05 00:00:00|     Fall|\n",
      "|            78|2017-01-11 00:00:00|   Winter|\n",
      "|            19|2016-09-27 00:00:00|     Fall|\n",
      "|            36|2016-10-27 00:00:00|     Fall|\n",
      "|            21|2016-09-30 00:00:00|     Fall|\n",
      "|            40|2017-02-04 00:00:00|   Winter|\n",
      "|            71|2016-07-07 00:00:00|   Summer|\n",
      "|             7|2016-09-24 00:00:00|     Fall|\n",
      "|            64|2017-01-26 00:00:00|   Winter|\n",
      "|            20|2017-04-30 00:00:00|   Spring|\n",
      "|            36|2017-02-03 00:00:00|   Winter|\n",
      "+--------------+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_frame_seasonsTickets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a tempView table for analysis on seasons\n",
    "d_frame_seasonsTickets.createOrReplaceTempView('temp_ViewSeasons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|NY_season|total_tickets|\n",
      "+---------+-------------+\n",
      "|   Spring|      2880687|\n",
      "|     Fall|      2830802|\n",
      "|   Summer|      2606208|\n",
      "|   Winter|      2485331|\n",
      "+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# season wise tickets\n",
    "spark.sql(\"Select NY_season, count(*) as total_tickets from temp_ViewSeasons group by NY_season order by total_tickets desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Spring:\n",
      "+---------+--------------+-------------+\n",
      "|NY_season|Violation_Code|total_tickets|\n",
      "+---------+--------------+-------------+\n",
      "|   Spring|            21|       402807|\n",
      "|   Spring|            36|       344834|\n",
      "|   Spring|            38|       271192|\n",
      "+---------+--------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "For Fall:\n",
      "+---------+--------------+-------------+\n",
      "|NY_season|Violation_Code|total_tickets|\n",
      "+---------+--------------+-------------+\n",
      "|     Fall|            36|       456046|\n",
      "|     Fall|            21|       357479|\n",
      "|     Fall|            38|       283828|\n",
      "+---------+--------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "For Summer:\n",
      "+---------+--------------+-------------+\n",
      "|NY_season|Violation_Code|total_tickets|\n",
      "+---------+--------------+-------------+\n",
      "|   Summer|            21|       405961|\n",
      "|   Summer|            38|       247561|\n",
      "|   Summer|            36|       240396|\n",
      "+---------+--------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "For Winter:\n",
      "+---------+--------------+-------------+\n",
      "|NY_season|Violation_Code|total_tickets|\n",
      "+---------+--------------+-------------+\n",
      "|   Winter|            21|       362341|\n",
      "|   Winter|            36|       359338|\n",
      "|   Winter|            38|       259723|\n",
      "+---------+--------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Top 3 common violations for each season as done in Que(4)\n",
    "\n",
    "print(\"For Spring:\")\n",
    "spark.sql(\"select NY_season, Violation_Code, count(*) as total_tickets from temp_ViewSeasons \\\n",
    "where NY_season = 'Spring' group by NY_season, Violation_Code order by total_tickets desc\").show(3)\n",
    "\n",
    "print(\"For Fall:\")\n",
    "spark.sql(\"select NY_season, Violation_Code, count(*) as total_tickets from temp_ViewSeasons \\\n",
    "where NY_season = 'Fall' group by NY_season, Violation_Code order by total_tickets desc\").show(3)\n",
    "\n",
    "print(\"For Summer:\")\n",
    "spark.sql(\"select NY_season, Violation_Code, count(*) as total_tickets from temp_ViewSeasons \\\n",
    "where NY_season = 'Summer' group by NY_season, Violation_Code order by total_tickets desc\").show(3)\n",
    "\n",
    "print(\"For Winter:\")\n",
    "spark.sql(\"select NY_season, Violation_Code, count(*) as total_tickets from temp_ViewSeasons \\\n",
    "where NY_season = 'Winter' group by NY_season, Violation_Code order by total_tickets desc\").show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  As per the result sets we could infer that the codes like '21','36','38' are the most common occuring Violation_Codes for each the seasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. The fines collected from all the instances of parking violation constitute a source of revenue for the NYC Police Department. \n",
    "   Letâ€™s take an example of estimating this for the three most commonly occurring codes.\n",
    "   Find the total occurrences of the three most common violation codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|Violation_Code|total_tickets|\n",
      "+--------------+-------------+\n",
      "|            21|      1528588|\n",
      "|            36|      1400614|\n",
      "|            38|      1062304|\n",
      "+--------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Violation_Code, count(*) as total_tickets from temp_ViewTickets \\\n",
    "                  group by Violation_Code order by total_tickets desc\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then, visit the website:\n",
    "1. http://www1.nyc.gov/site/finance/vehicles/services-violation-codes.page\n",
    "2. It lists the fines associated with different violation codes. \n",
    "3. Theyâ€™re divided into two categories: one for the highest-density locations in the city and the other for the rest of the city. For the sake of simplicity, take the average of the two.\n",
    "4. Using this information, find the total amount collected for the three violation codes with the maximum tickets. State the code that has the highest total collection.\n",
    "5. What can you intuitively infer from these findings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Code '21' whose Average_fine_amount = $55\n",
      "+--------------+--------+\n",
      "|Violation_Code|    fine|\n",
      "+--------------+--------+\n",
      "|            21|84072340|\n",
      "+--------------+--------+\n",
      "\n",
      "For Code '36' whose Average_fine_amount = $50\n",
      "+--------------+--------+\n",
      "|Violation_Code|    fine|\n",
      "+--------------+--------+\n",
      "|            36|70030700|\n",
      "+--------------+--------+\n",
      "\n",
      "For Code '38' whose Average_fine_amount = $50\n",
      "+--------------+--------+\n",
      "|Violation_Code|    fine|\n",
      "+--------------+--------+\n",
      "|            38|53115200|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checked the site and here are Avegrages for the codes as found above:        \n",
    "# Average fine amount for '21' = $55\n",
    "## Average fine amount for '36' = $50\n",
    "### Average fine amount for '38' = $50\n",
    "\n",
    "\n",
    "print(\"For Code '21' whose Average_fine_amount = $55\")\n",
    "spark.sql(\"select Violation_Code,(count(*) * 55) as fine from temp_ViewTickets \\\n",
    "                  where Violation_Code = 21 group by Violation_Code order by fine desc\").show()\n",
    "\n",
    "\n",
    "print(\"For Code '36' whose Average_fine_amount = $50\")\n",
    "spark.sql(\"select Violation_Code,(count(*) * 50) as fine from temp_ViewTickets \\\n",
    "                  where Violation_Code = 36 group by Violation_Code order by fine desc\").show()\n",
    "\n",
    "\n",
    "print(\"For Code '38' whose Average_fine_amount = $50\")\n",
    "spark.sql(\"select Violation_Code,(count(*) * 50) as fine from temp_ViewTickets \\\n",
    "                  where Violation_Code = 38 group by Violation_Code order by fine desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inference\n",
    "\n",
    "1. Total fine sums up to slightly greater than $207M.\n",
    "2. Violation_Code that has the highest total collection: 21 \n",
    "(~40% of total fine)\n",
    "3. Main reason for fine:\n",
    "(Street Cleaning: No parking where parking is not allowed by sign, street marking or traffic control device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
